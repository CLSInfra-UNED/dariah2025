{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODE FOR OBTAINING THE RESPOND OF LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n",
    "!pip install nervaluate==0.1.8\n",
    "!pip install transformers\n",
    "!pip install nervaluate\n",
    "!pip install langchain\n",
    "!pip install --upgrade langchain-together\n",
    "!pip install torch\n",
    "!pip install langchain_community\n",
    "!pip install langchain_huggingface\n",
    "!pip install together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from nervaluate import Evaluator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "import time\n",
    "import ast\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "from collections import defaultdict\n",
    "import time\n",
    "from langchain_together import ChatTogether\n",
    "import together\n",
    "from together import Together\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path='/Users/salvador/GitHub/absa-travelogues/ABSA_data_ok/'\n",
    "path_1='/Users/salvador/GitHub/absa-travelogues _deepseek/ABSA_data_results_deepseek/'\n",
    "#path1='/Users/salvador/Downloads/English_df_asp_null_nosent_century.csv'\n",
    "#antiguo path2\n",
    "path_source='/Users/salvador/GitHub/absa-travelogues/ABSA_data/English_asp_null_nosent_IOB_century_4.tsv'\n",
    "\n",
    "TOGETHER_API_KEY0 \"use your key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "link_modelo = Together(api_key=TOGETHER_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.- Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Century', 'text', 'sentence', 'annotation', 'aspect_cat', 'results',\n",
       "       'words', 'gold_labels'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cargo los datos y renombro las columnas. Los tados estan en English_asp_null_nosent_IOB_century_4.tsv\n",
    "\n",
    "lab_df_2=pd.read_csv(path_source,delimiter=\"\\t\",index_col=None)\n",
    "lab_df_2.rename(columns={'_sentence_text': 'sentence', 'sentence_split': 'words', 'iob':'gold_labels'}, inplace=True)\n",
    "lab_back=lab_df_2\n",
    "lab_back.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.-  Auxiliary FUNTIONS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chequeo de la longitud del prompt\n",
    "\"\"\" This funtion receives the systems prompt and the user promt together the\n",
    "prompt maixumun size  asociated to the model and establish if it is ok or it is needed\n",
    "to split the user prompt (TODO)\n",
    "Args: systems prompt, user promt, prompt limit\n",
    "return: True false\n",
    "\"\"\"\n",
    "def tamano_prompt(contenido_sys,contenido_user,modelo,prompt_limit):\n",
    "  from langchain_together import ChatTogether\n",
    "  llm = ChatTogether(\n",
    "      model=modelo,\n",
    "      temperature=0,\n",
    "      api_key=TOGETHER_API_KEY,\n",
    "  )\n",
    "  # la estrucutra de prompt es difernte a si usas la API directa de together\n",
    "  messages=[\n",
    "        (\n",
    "\n",
    "            \"system\", contenido_sys,\n",
    "        ),\n",
    "        (\n",
    "            \"human\",contenido_user,\n",
    "        ),\n",
    "    ]\n",
    "  ai_msg = llm.invoke(messages)\n",
    "  if (ai_msg.response_metadata['token_usage']['prompt_tokens']>prompt_limit):\n",
    "      LIMIT=True\n",
    "\n",
    "  else:\n",
    "      LIMIT=False\n",
    "  print(ai_msg.response_metadata['token_usage']['prompt_tokens'])\n",
    "  return LIMIT\n",
    "#ai_msg.usage_metadata\n",
    "#ai_msg.response_metadata['token_usage']['prompt_tokens']\n",
    "#mirar aqui la longitud de contexto. para llama3.1 8B es 128K, 70B 128K 405B, 4096\n",
    "\n",
    "def divide_prompt(contenido_user,promt_limit):\n",
    "  #TODO\n",
    "  lista_prompts=[]\n",
    "  if promt_limit>(128000/2):\n",
    "    #divido el contenido user las veces encesarias y lo meto en lalista\n",
    "    print()\n",
    "  lista_prompts.append(contenido_user)\n",
    "  return lista_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entitites(ent_dict):\n",
    "    all_values = []\n",
    "    list_category=[]\n",
    "    for category,sublist in ent_dict.items():\n",
    "        for value_list in sublist:\n",
    "            all_values.append(value_list)\n",
    "            list_category.append(category)\n",
    "    \n",
    "    return all_values,list_category\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sub_list(sl,l):\n",
    "    results=[]\n",
    "    sll=len(sl)\n",
    "    for ind in (i for i,e in enumerate(l) if e==sl[0]):\n",
    "        if l[ind:ind+sll]==sl:\n",
    "            results.append((ind,ind+sll-1))\n",
    "\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# El problema es  que cuando se pasa una frase a iob se usaba sentence split y esto no coindice con la sepracion de palabras. Luego hay que usar la separacion de palabras y no frase.\n",
    "def to_IOB(sentence, entity_list,category_list):\n",
    "    \n",
    "    #split sentence and make into list with O's\n",
    "    sentence_O=[[\"O\"] *len(sentence)][0]\n",
    "    sentence_1=[[\"O\"] *len(sentence)][0]\n",
    "    \n",
    "    #sentence_O = [[\"O\"] * len(sentence.split(\" \"))][0]\n",
    "    if len(entity_list) != 0: \n",
    "        cuenta=0\n",
    "        for ent in entity_list:\n",
    "            ent_ok = re.findall(r'\\w+|[^\\w\\s]', ent)\n",
    "            #esta línea filtra los signos de puntuación de la lista, permitiendo solo los elementos alfanuméricos y conservando los signos de puntuación que están en la lista ['.', ';',','].\n",
    "            ent_ok_filtrada = [elemento for elemento in ent_ok if elemento not in string.punctuation or elemento in ['.', ';',',']]\n",
    "\n",
    "            #find indices of sublist entity in list sentence\n",
    "            ind = find_sub_list(ent_ok_filtrada,sentence) \n",
    "          \n",
    "            #insert IOB-aspect pattern into sentence with O's\n",
    "            for result in ind:\n",
    "                #make list of numbers out of index information [(51,53)] = [51,52,53]\n",
    "                all_indices = list(range(result[0], result[-1]+1))    \n",
    "            #index of start aspect = B-aspect\n",
    "                start_index = all_indices[0]\n",
    "                sentence_O[start_index] = \"B-ASPECT\"\n",
    "                sentence_1[start_index] = f\"B-{category_list[cuenta].upper()}\"\n",
    "                \n",
    "\n",
    "            #index of other parts of the aspect = I-aspect\n",
    "                other_indices = all_indices[1:]\n",
    "                for other_ind in other_indices:\n",
    "                    sentence_O[other_ind] = \"I-ASPECT\"\n",
    "                    sentence_1[other_ind] = f\"I-{category_list[cuenta].upper()}\"\n",
    "                    \n",
    "            cuenta +=1   \n",
    "        #print(sentence_O)\n",
    "        #print(sentence_1)\n",
    "    return sentence_O, sentence_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.- LLM SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#context length=131072 FP8\n",
    "#modelo=\"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\"\n",
    "#context length=131072 FP8\n",
    "modelo=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\"\n",
    "#context length=8192 FP16\n",
    "#modelo=\"meta-llama/Llama-3-8b-chat-hf\"\n",
    "#context length=131072 FP16\n",
    "#modelo=\"meta-llama/Llama-3.2-3B-Instruct-Turbo\"\n",
    "#context length=131072 FP16\n",
    "modelo='meta-llama/Llama-3.3-70B-Instruct-Turbo'\n",
    "modelo='deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free'\n",
    "#modelo='meta-llama/Llama-3.3-70B-Instruct-Turbo-Free'\n",
    "modelo='meta-llama/Llama-3.3-70B-Instruct-Turbo'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.- Aspect extration on travel literature data \n",
    "\n",
    "\n",
    "- Output to IOB-format for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1- Prompting : Los tres prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Añadir ejemplos con listas con varios elementos\n",
    "##### Eliminar clave \"entities\" del diccionario\n",
    "##### Mejorar la salida del json en caso de error, volver a pedir\n",
    "##### Intentar mejorar la consistencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "question = \"Extract the relevant named entities from the given sentence.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "contenido_sys = \"You are a historian and literary scholar with expertise on historical travel literature, colonial literature and labelling named entities\"\n",
    "\n",
    "one_shot=\"\"\"Task: Extract relevant named entities from the given sentence based on the following labels:\n",
    "- person: proper names of people\n",
    "- location: proper names of locations\n",
    "- fauna: common and scientific names of animals and fauna\n",
    "- flora: common and scientific names of plants and flora\n",
    "- weather: descriptions of weather phenomena and climate\n",
    "- organisation: proper names of organizations\n",
    "- biome: names of biomes\n",
    "- myth: fantastical entities, deities, or God\n",
    "- hum_landform: landforms influenced by people\n",
    "- nat_landform: landforms not influenced by people\n",
    "- nat_phenomenon: dynamic and naturally occurring phenomena\n",
    "- land_cover: elements that cover the land\n",
    "\n",
    "Only respond in JSON format, Do not add any more comment.\n",
    "The structure of the JSON format is like this:\n",
    "{\n",
    "        \"person\": [\"entity_1\", \"entity_2\", \"entity_n\"],\n",
    "        \"location\": [\"entity_1\", \"entity_2\", \"entity_n\"],\n",
    "        \"fauna\": [\"entity_1\", \"entity_2\", \"entity_n\"],\n",
    "        \"flora\": [\"entity_1\", \"entity_2\", \"entity_n\"],\n",
    "        \"weather\": [\"entity_1\", \"entity_2\", \"entity_n\"],\n",
    "        \"organisation\": [\"entity_1\", \"entity_2\", \"entity_n\"],\n",
    "        \"biome\": [\"entity_1\", \"entity_2\", \"entity_n\"],\n",
    "        \"myth\": [\"entity_1\", \"entity_2\", \"entity_n\"],\n",
    "        \"hum_landform\": [\"entity_1\", \"entity_2\", \"entity_n\"],\n",
    "        \"nat_landform\": [\"entity_1\", \"entity_2\", \"entity_n\"],\n",
    "        \"nat_phenomenon\": [\"entity_1\", \"entity_2\", \"entity_n\"],\n",
    "        \"land_cover\": [\"entity_1\", \"entity_2\", \"entity_n\"],\n",
    "        }\n",
    "        \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "few_shot= f\"\"\"{one_shot} \n",
    "Here you have some examples:\n",
    "Example1:\n",
    "Sentence: <<<Hurricane Katrina devastated New Orleans in 2005, causing widespread flooding and destruction.>>>\n",
    "Output:\n",
    "   {{\"person\": [],\n",
    "    \"location\": [\"New Orleans\"],\n",
    "    \"fauna\": [],\n",
    "    \"flora\": [],\n",
    "    \"weather\": [\"Hurricane\\\\'s' Katrina\"],\n",
    "    \"organisation\": [],\n",
    "    \"biome\": [],\n",
    "    \"myth\": [],\n",
    "    \"hum_landform\": [],\n",
    "    \"nat_landform\": [],\n",
    "    \"nat_phenomenon\": ['flooding'],\n",
    "    \"land_cover\": [],\n",
    "  }}\n",
    "  \n",
    "Example 2:\n",
    "Sentence: <<<The Great Wall of China stretches across the Gobi Desert, where Bactrian camels roam freely.>>>\n",
    "Output:\n",
    "   {{\"person\": [],\n",
    "    \"location\": [],\n",
    "    \"fauna\": [\"Bactrian camels\"],\n",
    "    \"flora\": [],\n",
    "    \"weather\": [],\n",
    "    \"organisation\": [],\n",
    "    \"biome\": [\"Gobi Desert\"],\n",
    "    \"myth\": [],\n",
    "    \"hum_landform\": [\"Great Wall of China\"],\n",
    "    \"nat_landform\": [],\n",
    "    \"nat_phenomenon\": [],\n",
    "    \"land_cover\": [],\n",
    "  }} \n",
    "Example 3:\n",
    "Sentence: <<<The Amazon rainforest is home to jaguars and giant water lilies, while the Andes Mountains tower above.>>>\n",
    "Output:\n",
    "   {{\"person\": [],\n",
    "    \"location\": [],\n",
    "    \"fauna\": [\"jaguars\"],\n",
    "    \"flora\": [\"giant water lilies\"],\n",
    "    \"weather\": [],\n",
    "    \"organisation\": [],\n",
    "    \"biome\": [\"Amazon rainforest\"],\n",
    "    \"myth\": [],\n",
    "    \"hum_landform\": [],\n",
    "    \"nat_landform\": [\"Andes Mountains\"],\n",
    "    \"nat_phenomenon\": [],\n",
    "    \"land_cover\": [],\n",
    "   }}\n",
    "  \"\"\"\n",
    "chain_of_thought= f\"\"\"{one_shot}\n",
    "Let's approach this step-by-step:\n",
    "Example 1:\n",
    "Sentence: <<<The Amazon rainforest is home to jaguars and giant water lilies, while the Andes Mountains tower above.>>>\n",
    "\n",
    "Step 1: Identify potential named entities\n",
    "- Amazon rainforest\n",
    "- jaguars\n",
    "- giant water lilies\n",
    "- Andes Mountains\n",
    "\n",
    "Step 2: Categorize each entity and asign a label\n",
    "- Amazon rainforest: biome (rainforest is a type of biome)\n",
    "- jaguars: fauna (common name of an animal)\n",
    "- giant water lilies: flora (common name of a plant)\n",
    "- Andes Mountains: nat_landform (mountain range not influenced by people)\n",
    "\n",
    "Step 3: Format the output\n",
    "\n",
    "   {{\"person\": [],\n",
    "    \"location\": [],\n",
    "    \"fauna\": [\"jaguars\"],\n",
    "    \"flora\": [\"giant water lilies\"],\n",
    "    \"weather\": [],\n",
    "    \"organisation\": [],\n",
    "    \"biome\": [\"Amazon rainforest\"],\n",
    "    \"myth\": [],\n",
    "    \"hum_landform\": [],\n",
    "    \"nat_landform\": [\"Andes Mountains\"],\n",
    "    \"nat_phenomenon\": [],\n",
    "    \"land_cover\": [],\n",
    "   }}\n",
    "\n",
    "Example 2:\n",
    "Sentence: <<<Hurricane Katrina devastated New Orleans in 2005, causing widespread flooding and destruction.>>>\n",
    "\n",
    "Step 1: Identify potential named entities\n",
    "- Hurricane Katrina\n",
    "- New Orleans\n",
    "\n",
    "Step 2: Categorize each entity and asign a label\n",
    "- Hurricane's Katrina: weather (description of a weather event)\n",
    "- New Orleans: location (proper name of a city)\n",
    "\n",
    "Step 3: Format the output\n",
    "\n",
    "  { {\"person\": [],\n",
    "    \"location\": [\"New Orleans\"],\n",
    "    \"fauna\": [],\n",
    "    \"flora\": [],\n",
    "    \"weather\": [\"Hurricane Katrina\"],\n",
    "    \"organisation\": [],\n",
    "    \"biome\": [],\n",
    "    \"myth\": [],\n",
    "    \"hum_landform\": [],\n",
    "    \"nat_landform\": [],\n",
    "    \"nat_phenomenon\": [],\n",
    "    \"land_cover\": [],\n",
    "  }}\n",
    "\n",
    "Example 3:\n",
    "Sentence: <<<The Great Wall of China stretches across the Gobi Desert, where Bactrian camels roam freely.>>>\n",
    "\n",
    "Step 1: Identify potential named entities\n",
    "- Great Wall of China\n",
    "- Gobi Desert\n",
    "- Bactrian camels\n",
    "\n",
    "Step 2: Categorize each entity and asign a label\n",
    "- Great Wall of China: hum_landform (man-made structure influencing the landscape)\n",
    "- Gobi Desert: biome (desert is a type of biome)\n",
    "- Bactrian camels: fauna (common name of an animal)\n",
    "\n",
    "Step 3: Format the output\n",
    "\n",
    "   {{\"person\": [],\n",
    "    \"location\": [],\n",
    "    \"fauna\": [\"Bactrian camels\"],\n",
    "    \"flora\": [],\n",
    "    \"weather\": [],\n",
    "    \"organisation\": [],\n",
    "    \"biome\": [\"Gobi Desert\"],\n",
    "    \"myth\": [],\n",
    "    \"hum_landform\": [\"Great Wall of China\"],\n",
    "    \"nat_landform\": [],\n",
    "    \"nat_phenomenon\": [],\n",
    "    \"land_cover\": [],\n",
    "  }}\n",
    "\n",
    "Now, DO NOT take into account the previous examples, use them JUST AS A REFERENCE. \n",
    "\n",
    "The sentence for analyzing is:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def llm_inference(modelo,mensaje):\n",
    "  \n",
    "  link_modelo = Together(api_key=TOGETHER_API_KEY)\n",
    "  output =[]\n",
    "  extract = link_modelo.chat.completions.create(\n",
    "      messages=mensaje,\n",
    "      model=modelo,\n",
    "      #top_p=1,\n",
    "      #top_k=40,\n",
    "      temperature= 0.0,\n",
    "      #response_format={\n",
    "      #   \"type\": \"json_object\",\n",
    "      #   \"schema\": characters.model_json_schema(),},\n",
    "  )\n",
    "  try:\n",
    "    output = json.loads(extract.choices[0].message.content)\n",
    "  except ValueError as e:\n",
    "    print ('Error en JSON tipo : ',e)\n",
    "    print(output)\n",
    "    extract = link_modelo.chat.completions.create(\n",
    "              messages=[\n",
    "                  {\n",
    "                      \"role\": \"system\",\n",
    "                      \"content\": ' You are a helpfull assistant expert on JSON format.Your work is to correct this JSON to be a correct JSON text.Only repond in a JSON format. Do not add any more comment.',\n",
    "                  },\n",
    "                  {\n",
    "                      \"role\": \"user\",\n",
    "                      \"content\":f'This is the uncorrect JSON text:  {extract.choices[0].message.content}',\n",
    "                  },\n",
    "              ],\n",
    "              model=modelo,\n",
    "              temperature= 0.0,\n",
    "            )\n",
    "    try:\n",
    "          output = json.loads(extract.choices[0].message.content)\n",
    "    except ValueError as e:\n",
    "        print (f'Segundo  error, skipe Sentence. Tipo de error: ',e)\n",
    "        output=[]\n",
    "\n",
    "  #print(extract.choices[0].message.content)\n",
    "  #print(json.dumps(output, indent=1))\n",
    "  #print(output)\n",
    "  return(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3.- Checking DATA Structure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coincide el words y gold labels 2151\n",
      "coinciden TODOS 1898\n",
      "coincide el words y gold labels 0\n",
      "total 2151\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'print(\\'palabras\\',len(ll1))\\nprint(\\'split sentence\\',len(lab_df_1[\\'sentence\\'][num].split(\" \")))\\nprint(lab_df_1.loc[num,\\'gold_labels\\'])\\nprint(lab_df_1.loc[num,\\'words\\'].split(\",\"))\\nprint(lab_df_1[\\'sentence\\'][num].split(\" \"))\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ESTE ES EL CODIGO DE COMPROBACION DE LONGITUDES DE WORDS Y IOB LABELS DEL DATASET . ES UNA COMPROBACION QUE SE HACE ANTES DE EJECUTAR LA INFERENCIA\n",
    "count=0\n",
    "count_1 =0\n",
    "count_2 =0\n",
    "diferente=[]\n",
    "\n",
    "for n in range(lab_back.shape[0]):\n",
    "    ll=ast.literal_eval(lab_back.loc[n,'gold_labels'])\n",
    "    ll1=ast.literal_eval(lab_back.loc[n,'words'])\n",
    "    sentence=re.findall(r'\\w+|[^\\w\\s]', lab_back['sentence'][n])\n",
    "    lista_filtrada = [elemento for elemento in sentence if elemento not in string.punctuation or elemento in ['.', ';',',']]\n",
    "\n",
    "    if len(ll)==len(ll1):     \n",
    "        if len(ll)==len(lista_filtrada):  \n",
    "            count_1 +=1\n",
    "        else:\n",
    "            diferente.append(count) \n",
    "        count +=1\n",
    "        \n",
    "             \n",
    "    else:\n",
    "        if len(ll)==len(lista_filtrada):\n",
    "            count_2 +=1\n",
    "\n",
    "print('coincide el words y gold labels',count)\n",
    "print('coinciden TODOS',count_1)\n",
    "print('coincide el words y gold labels', count_2)\n",
    "print('total',lab_back.shape[0])\n",
    "\n",
    "\"\"\"print('palabras',len(ll1))\n",
    "print('split sentence',len(lab_df_1['sentence'][num].split(\" \")))\n",
    "print(lab_df_1.loc[num,'gold_labels'])\n",
    "print(lab_df_1.loc[num,'words'].split(\",\"))\n",
    "print(lab_df_1['sentence'][num].split(\" \"))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4.-  ASPECT EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ESTE ES EL CODIGO DE EXTRACCION\n",
    "#path_1='/Users/salvador/GitHub/absa-travelogues/ABSA_data_ok/resultado1/'\n",
    "\n",
    "#def busca_aspect(path_1,lab_back,modelo):\n",
    "    #list_aspect=[]\n",
    "    #list_category_aspect=[]\n",
    "#lab_back.loc[:,'ASPECT'] = None \n",
    "#lab_back.loc[:,'CAT_ASPECT'] = None\n",
    "\n",
    "\n",
    "prompts = {\n",
    "    'one_shot': one_shot,\n",
    "    'few_shot': few_shot,\n",
    "    'chain_of_thought': chain_of_thought\n",
    "}\n",
    "print('MODELO EN EJECUCION',modelo)\n",
    "for prompt,prompt_content in prompts.items():\n",
    "    lab_back=lab_df_2 \n",
    "    print('Resultado de ',prompt)\n",
    "    cuenta=0\n",
    "    errores=0\n",
    "    lista_errores=[]\n",
    "    for sentence in lab_back['sentence']:\n",
    "        print(' Frase=',cuenta)\n",
    "# para cada sentence que tengo en el dataset \n",
    "    #for sentence in lab_back['sentence'][cuenta:]:   \n",
    "        contenido_user=''\n",
    "        result=[]\n",
    "        contenido_user=f'{prompt_content}'+f\"<<< {sentence} >>>\"\n",
    "        messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": contenido_sys,\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\":contenido_user,\n",
    "                },\n",
    "            ]\n",
    "        result=llm_inference(modelo,messages)\n",
    "        #devuelve dos listas aspect y category_aspect indicando en una que es ASPECT y en la otra la categoria e.g. PERSON\n",
    "        #print(result)\n",
    "        if result!=[]:\n",
    "            aspect,category_aspect=extract_entitites(result)\n",
    "            #print(' aspect',aspect)\n",
    "            #print(' sentence',sentence)\n",
    "            #print('categoria aspect',category_aspect)\n",
    "            #convertimos un string con las palabras anotadas a mano como si fuera una lista en una lista con la funcion ast.literal_eval\n",
    "        else:\n",
    "            print('NO result')\n",
    "            aspect=[]\n",
    "            category_aspect=[]\n",
    "            errores +=1\n",
    "            lista_errores.append(cuenta)\n",
    "            \n",
    "        etiqueta_aspect,etiqueta_categoria=to_IOB(ast.literal_eval(lab_back['words'][cuenta]),aspect,category_aspect)   \n",
    "        #list_aspect.append(etiqueta_aspect)\n",
    "        #list_category_aspect.append(etiqueta_categoria)\n",
    "        #print('procesado',cuenta)\n",
    "        lab_back.loc[cuenta,'ASPECT']=','.join(etiqueta_aspect)\n",
    "        lab_back.loc[cuenta,'CAT_ASPECT']=','.join(etiqueta_categoria)\n",
    "        if cuenta % 100 == 0:\n",
    "            lab_back.to_csv(path_1+f'{prompt}_english_{cuenta}_ok.csv', index=False) \n",
    "            print(f'Datos guardados en la iteración {cuenta}')\n",
    "        cuenta +=1\n",
    "    lab_back.to_csv(path_1+f'{prompt}_english_fin_ok.csv', index=False) \n",
    "    #return lab_back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.-OLD CODE NOT USE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CARGO LOS DATOS Y LO CONVIERTO EN LISTA DE LISTAS\n",
    "#\"Coarse grey mud , superficial layer more liquid and brown\n",
    "#OJO QUE HAY DOS FORMATOS DIFERNTES EL TRUE Y EL RESTO\n",
    "res_ok=pd.read_csv('/Users/salvador/GitHub/absa-travelogues/ABSA_data_ok/resultado_ok_century/chain_of_thought_english_fin_ok.csv')\n",
    "\n",
    "true = res_ok[\"gold_labels\"].to_list()\n",
    "true1=[ast.literal_eval(string) for string in true]\n",
    "predicted = res_ok[\"ASPECT\"].to_list()\n",
    "predicted1=[string.split(',') for string in predicted]\n",
    "predicted_cat=res_ok[\"CAT_ASPECT\"].to_list()\n",
    "predicted_cat1=[string.split(',') for string in  predicted_cat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = res_ok[\"gold_labels\"].to_list()\n",
    "true1=[ast.literal_eval(string) for string in true]\n",
    "predicted = res_ok[\"ASPECT\"].to_list()\n",
    "predicted1=[string.split(',') for string in predicted]\n",
    "predicted_cat=res_ok[\"CAT_ASPECT\"].to_list()\n",
    "predicted_cat1=[string.split(',') for string in  predicted_cat]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that everything is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of true is list: True\n",
      "Type of predicted is list: True\n",
      "Type of true[0] is list: True\n",
      "Type of predicted[0] is list: True\n",
      "Type of predicted_cast[0] is list: True\n"
     ]
    }
   ],
   "source": [
    "print(\"Type of true is list:\", isinstance(true1, list))\n",
    "print(\"Type of predicted is list:\", isinstance(predicted1, list))\n",
    "print(\"Type of true[0] is list:\", isinstance(true1[0], list))\n",
    "print(\"Type of predicted[0] is list:\", isinstance(predicted1[0], list))\n",
    "print(\"Type of predicted_cast[0] is list:\", isinstance(predicted_cat1[0], list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sequence in predicted_cat1:\n",
    "    for label in sequence:\n",
    "        if not isinstance(label, str):\n",
    "            print(label, type(label))  # Each label should be a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sequence in true1:\n",
    "    for label in sequence:\n",
    "        if not isinstance(label, str):\n",
    "            print(label, type(label))  # Each label should be a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sequence in predicted1:\n",
    "    for label in sequence:\n",
    "        if not isinstance(label, str):\n",
    "            print(label, type(label))  # Each label should be a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-FAUNA', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "num=26\n",
    "print(true1[num])\n",
    "print(predicted1[num])\n",
    "print(predicted_cat1[num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_back.to_csv(path_1+f'chain_of_thought_english_fin_ok.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-PERSON',\n",
       " 'I-PERSON',\n",
       " 'I-PERSON',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-PERSON',\n",
       " 'I-PERSON',\n",
       " 'I-PERSON',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-HUM_LANDFORM',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tags=['PERSON', 'LOCATION','FAUNA','FLORA', 'WEATHER_PHENOMENA', 'ORGANISATION','BIOMES','MYTHOLOGICAL_ENTITIES','HUMAN_LANDFORM','NATURAL_LANDFORM','NATURAL_PHENOMENON','LAND_COVER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator(true1, predicted_cat1, tags=['PERSON', 'LOCATION','FAUNA','FLORA', 'WEATHER_PHENOMENA', 'ORGANISATION','BIOMES','MYTHOLOGICAL_ENTITIES','HUMAN_LANDFORM','NATURAL_LANDFORM','NATURAL_PHENOMENON','LAND_COVER'], loader=\"list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c,results_by_tag =evaluator.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Century', 'text', 'sentence', 'annotation', 'aspect_cat', 'results',\n",
       "       'words', 'gold_labels', 'ASPECT', 'CAT_ASPECT'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_ok.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wikipedia-api\n",
      "  Downloading wikipedia_api-0.7.1.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: requests in /Users/salvador/opt/anaconda3/envs/neurips-llm/lib/python3.10/site-packages (from wikipedia-api) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/salvador/opt/anaconda3/envs/neurips-llm/lib/python3.10/site-packages (from requests->wikipedia-api) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/salvador/opt/anaconda3/envs/neurips-llm/lib/python3.10/site-packages (from requests->wikipedia-api) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/salvador/opt/anaconda3/envs/neurips-llm/lib/python3.10/site-packages (from requests->wikipedia-api) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/salvador/opt/anaconda3/envs/neurips-llm/lib/python3.10/site-packages (from requests->wikipedia-api) (2023.7.22)\n",
      "Building wheels for collected packages: wikipedia-api\n",
      "  Building wheel for wikipedia-api (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wikipedia-api: filename=Wikipedia_API-0.7.1-py3-none-any.whl size=14346 sha256=bca477ad66c1d16bb02e3d3fc338ca01c4ce7eb32785c45ec20873a8186df658\n",
      "  Stored in directory: /Users/salvador/Library/Caches/pip/wheels/4c/96/18/b9201cc3e8b47b02b510460210cfd832ccf10c0c4dd0522962\n",
      "Successfully built wikipedia-api\n",
      "Installing collected packages: wikipedia-api\n",
      "Successfully installed wikipedia-api-0.7.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install wikipedia-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/salvador/opt/anaconda3/envs/neurips-llm/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/salvador/opt/anaconda3/envs/neurips-llm/bin/python'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "wiki_wiki = wikipediaapi.Wikipedia(user_agent='Travel_project(sros@scc.uned.es)',language='es',extract_format=wikipediaapi.ExtractFormat.WIKI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page - Exists: False\n",
      "Page - Title: dolor de boca\n",
      "Page - Summary: \n"
     ]
    }
   ],
   "source": [
    "page_py = wiki_wiki.page('dolor ')\n",
    "print(\"Page - Exists: %s\" % page_py.exists())\n",
    "print(\"Page - Title: %s\" % page_py.title)\n",
    "print(\"Page - Summary: %s\" % page_py.summary[0:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sections(sections, level=0):\n",
    "        for s in sections:\n",
    "                print(\"%s: %s - %s\" % (\"*\" * (level + 1), s.title, s.text[0:40]))\n",
    "                print_sections(s.sections, level + 1)\n",
    "print_sections(page_py.sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'type' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[161], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwikipediaapi\u001b[39;00m\n\u001b[1;32m      2\u001b[0m wiki_wiki \u001b[38;5;241m=\u001b[39m wikipediaapi\u001b[38;5;241m.\u001b[39mWikipedia(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMyProjectName (merlin@example.com)\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/deep/lib/python3.8/site-packages/wikipediaapi/__init__.py:30\u001b[0m\n\u001b[1;32m     26\u001b[0m log \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# https://www.mediawiki.org/wiki/API:Main_page\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m PagesDict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWikipediaPage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mExtractFormat\u001b[39;00m(IntEnum):\n\u001b[1;32m     34\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Represents extraction format.\"\"\"\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'type' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "\n",
    "HUGGINGFACEHUB_API_TOKEN = \"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\"\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACEHUB_API_TOKEN\n",
    "import wikipediaapi\n",
    "wiki_wiki = wikipediaapi.Wikipedia('MyProjectName (merlin@example.com)', 'en')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Esultado: {'person': [], 'location': [], 'fauna': ['plankton'], 'flora': [], 'weather': [], 'organisation': [], 'biome': [], 'myth': [], 'hum_landform': [], 'nat_landform': [], 'nat_phenomenon': [], 'land_cover': []}\n",
    "categoria aspect ['fauna']\n",
    "entidad: ['plankton']\n",
    "entidad filtrada: ['plankton']\n",
    "etiqueta_aspect ['O', 'O']\n",
    "etiqueta_categoria ['O', 'O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=['Plankton']\n",
    "categoria_aspect= ['fauna']\n",
    "entidad=['plankton']\n",
    "entidad=['plankton']\n",
    "ind=find_sub_list(entidad,sentence)\n",
    "sentence=['Plankton']\n",
    "entidad=['plankton']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entidad=['B', '.', 'arenosa']\n",
    "sentence=['It', 'is' ,'very', 'closely' ,'allied', 'to' ,'B','.', 'arenosa','Goodsir']\n",
    "entity_list=entidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "entidad=['B', '.', 'arenosa']\n",
    "sentence=['It', 'is' ,'very', 'closely' ,'allied', 'to' ,'B','.', 'arenosa','Goodsir']\n",
    "sentence=['Plankton']\n",
    "entidad=['Plankton']    \n",
    "def find_sub_list(ent_1,sentence_1):\n",
    "    results=[]\n",
    "    sll=len(ent_1)\n",
    "    for ind in (i for i,e in enumerate(sentence_1) if e.upper()==ent_1[0].upper()):\n",
    "        if [elem.lower() for elem in sentence_1[ind:ind+sll]] == [elem.lower() for elem in ent_1]:\n",
    "            results.append((ind,ind+sll-1))\n",
    "    return results\n",
    "    \n",
    "ind=find_sub_list(entidad,sentence)\n",
    "print(ind)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "entity_list=['B', '.', 'arenosa']\n",
    "sentence=['It', 'is' ,'very', 'closely' ,'allied', 'to' ,'B','.', 'arenosa','Goodsir']\n",
    "for ent in entity_list:\n",
    "            ent_ok = re.findall(r\"\\w+|[^\\w\\s]\", ent)\n",
    "            # esta línea filtra los signos de puntuación de la lista, permitiendo solo los elementos alfanuméricos y conservando los signos de puntuación que están en la lista ['.', ';',','].\n",
    "            ent_ok_filtrada = [\n",
    "                elemento\n",
    "                for elemento in ent_ok\n",
    "                if elemento not in string.punctuation or elemento in [\".\", \";\", \",\"]\n",
    "            ]\n",
    "            print('entidad:',ent_ok)\n",
    "            print('entidad filtrada:',ent_ok_filtrada)\n",
    "            # find indices of sublist entity in list sentence\n",
    "            ind = find_sub_list(ent_ok_filtrada, sentence)\n",
    "            print('indices',ind)\n",
    "            # insert IOB-aspect pattern into sentence with O's\n",
    "            for result in ind:\n",
    "                # make list of numbers out of index information [(51,53)] = [51,52,53]\n",
    "                all_indices = list(range(result[0], result[-1] + 1))\n",
    "                # index of start aspect = B-aspect\n",
    "                start_index = all_indices[0]\n",
    "                sentence_O[start_index] = \"B-ASPECT\"\n",
    "                sentence_1[start_index] = f\"B-{category_list[cuenta].upper()}\"\n",
    "\n",
    "                # index of other parts of the aspect = I-aspect\n",
    "                other_indices = all_indices[1:]\n",
    "                for other_ind in other_indices:\n",
    "                    sentence_O[other_ind] = \"I-ASPECT\"\n",
    "                    sentence_1[other_ind] = f\"I-{category_list[cuenta].upper()}\"\n",
    "\n",
    "            cuenta += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_O=[[\"O\"] *len(sentence)][0]\n",
    "sentence_1=[[\"O\"] *len(sentence)][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B', '.', 'arenosa']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B']\n",
      "['B']\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "['.']\n",
      "['.']\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "['arenosa']\n",
      "['arenosa']\n",
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "    sentence_O=[[\"O\"] *len(sentence)][0]\n",
    "    sentence_1=[[\"O\"] *len(sentence)][0]\n",
    "    \n",
    "    #sentence_O = [[\"O\"] * len(sentence.split(\" \"))][0]\n",
    "    if len(entity_list) != 0: \n",
    "        cuenta=0\n",
    "        for ent in entity_list:\n",
    "            ent_ok = re.findall(r'\\w+|[^\\w\\s]', ent)\n",
    "            print(ent_ok)\n",
    "            #esta línea filtra los signos de puntuación de la lista, permitiendo solo los elementos alfanuméricos y conservando los signos de puntuación que están en la lista ['.', ';',','].\n",
    "            ent_ok_filtrada = [elemento for elemento in ent_ok if elemento not in string.punctuation or elemento in ['.', ';',',']]\n",
    "            print(ent_ok_filtrada)\n",
    "            print(type(ent_ok_filtrada))\n",
    "            print(type(sentence))\n",
    "            #find indices of sublist entity in list sentence\n",
    "            ind = find_sub_list(ent_ok_filtrada,sentence) \n",
    "          \n",
    "            #insert IOB-aspect pattern into sentence with O's\n",
    "            for result in ind:\n",
    "                #make list of numbers out of index information [(51,53)] = [51,52,53]\n",
    "                all_indices = list(range(result[0], result[-1]+1))    \n",
    "            #index of start aspect = B-aspect\n",
    "                start_index = all_indices[0]\n",
    "                sentence_O[start_index] = \"B-ASPECT\"\n",
    "                sentence_1[start_index] = f\"B-dato\"\n",
    "                \n",
    "\n",
    "            #index of other parts of the aspect = I-aspect\n",
    "                other_indices = all_indices[1:]\n",
    "                for other_ind in other_indices:\n",
    "                    sentence_O[other_ind] = \"I-ASPECT\"\n",
    "                    sentence_1[other_ind] = f\"I-dato\"\n",
    "                    \n",
    "            cuenta +=1   \n",
    "        #print(sentence_O)\n",
    "        #print(sentence_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ASPECT', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-dato', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(sentence_O)\n",
    "print(sentence_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "dato={\"person\": [],\n",
    "    \"location\": [],\n",
    "    \"fauna\": [\"jaguars\"],\n",
    "    \"flora\": [\"giant water lilies\"],\n",
    "    \"weather_phenomena\": [],\n",
    "    \"organisation\": [],\n",
    "    \"biomes\": [\"Amazon rainforest\"],\n",
    "    \"mythological_entities\": [],\n",
    "    \"human_landform\": [],\n",
    "    \"natural_landform\": [\"Andes Mountains\"],\n",
    "    \"natural_phenomenon\": [],\n",
    "    \"land_cover\": [],\n",
    "   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b=extract_entitites(dato)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fauna', 'flora', 'biomes', 'natural_landform']\n"
     ]
    }
   ],
   "source": [
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "contenido_sys = \"You are a historian and literary scholar with expertise on historical travel literature, colonial literature and labelling named entities\"\n",
    "\n",
    "one_shot=\"\"\"Task: Extract relevant named entities from the given sentence based on the following labels:\n",
    "- person: proper names of people\n",
    "- location: proper names of locations\n",
    "- fauna: common and scientific names of animals and fauna\n",
    "- flora: common and scientific names of plants and flora\n",
    "- weather: descriptions of weather phenomena and climate\n",
    "- organisation: proper names of organizations\n",
    "- biome: names of biomes\n",
    "- myth: fantastical entities, deities, or God\n",
    "- hum_landform: landforms influenced by people\n",
    "- nat_landform: landforms not influenced by people\n",
    "- nat_phenomenon: dynamic and naturally occurring phenomena\n",
    "- land_cover: elements that cover the land\n",
    "\n",
    "Only respond in JSON format, Do not add any more comment.\n",
    "The structure of the JSON format is like this:\n",
    "{\n",
    "        \"person\": [\"entity_1\", \"entity_2\", \"entity_n\"],\n",
    "        \"location\": [\"entity_1\", \"entity_2\", \"entity_n\"],\n",
    "        \"fauna\": [\"entity_1\", \"entity_2\", \"entity_n\"],\n",
    "        \"flora\": [\"entity_1\", \"entity_2\", \"entity_n\"],\n",
    "        \"weather\": [\"entity_1\", \"entity_2\", \"entity_n\"],\n",
    "        \"organisation\": [\"entity_1\", \"entity_2\", \"entity_n\"],\n",
    "        \"biome\": [\"entity_1\", \"entity_2\", \"entity_n\"],\n",
    "        \"myth\": [\"entity_1\", \"entity_2\", \"entity_n\"],\n",
    "        \"hum_landform\": [\"entity_1\", \"entity_2\", \"entity_n\"],\n",
    "        \"nat_landform\": [\"entity_1\", \"entity_2\", \"entity_n\"],\n",
    "        \"nat_phenomenon\": [\"entity_1\", \"entity_2\", \"entity_n\"],\n",
    "        \"land_cover\": [\"entity_1\", \"entity_2\", \"entity_n\"],\n",
    "        }\n",
    "        \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "few_shot= f\"\"\"{one_shot} \n",
    "Here you have some examples:\n",
    "Example1:\n",
    "Sentence: <<<Hurricane Katrina devastated New Orleans in 2005, causing widespread flooding and destruction.>>>\n",
    "Output:\n",
    "   {{\"person\": [],\n",
    "    \"location\": [\"New Orleans\"],\n",
    "    \"fauna\": [],\n",
    "    \"flora\": [],\n",
    "    \"weather\": [\"Hurricane\\\\'s' Katrina\"],\n",
    "    \"organisation\": [],\n",
    "    \"biome\": [],\n",
    "    \"myth\": [],\n",
    "    \"hum_landform\": [],\n",
    "    \"nat_landform\": [],\n",
    "    \"nat_phenomenon\": ['flooding'],\n",
    "    \"land_cover\": [],\n",
    "  }}\n",
    "  \n",
    "Example 2:\n",
    "Sentence: <<<The Great Wall of China stretches across the Gobi Desert, where Bactrian camels roam freely.>>>\n",
    "Output:\n",
    "   {{\"person\": [],\n",
    "    \"location\": [],\n",
    "    \"fauna\": [\"Bactrian camels\"],\n",
    "    \"flora\": [],\n",
    "    \"weather\": [],\n",
    "    \"organisation\": [],\n",
    "    \"biome\": [\"Gobi Desert\"],\n",
    "    \"myth\": [],\n",
    "    \"hum_landform\": [\"Great Wall of China\"],\n",
    "    \"nat_landform\": [],\n",
    "    \"nat_phenomenon\": [],\n",
    "    \"land_cover\": [],\n",
    "  }} \n",
    "Example 3:\n",
    "Sentence: <<<The Amazon rainforest is home to jaguars and giant water lilies, while the Andes Mountains tower above.>>>\n",
    "Output:\n",
    "   {{\"person\": [],\n",
    "    \"location\": [],\n",
    "    \"fauna\": [\"jaguars\"],\n",
    "    \"flora\": [\"giant water lilies\"],\n",
    "    \"weather\": [],\n",
    "    \"organisation\": [],\n",
    "    \"biome\": [\"Amazon rainforest\"],\n",
    "    \"myth\": [],\n",
    "    \"hum_landform\": [],\n",
    "    \"nat_landform\": [\"Andes Mountains\"],\n",
    "    \"nat_phenomenon\": [],\n",
    "    \"land_cover\": [],\n",
    "   }}\n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_of_thought= f\"\"\"{one_shot}\n",
    "Let's approach this step-by-step:\n",
    "Example 1:\n",
    "Sentence: <<<The Amazon rainforest is home to jaguars and giant water lilies, while the Andes Mountains tower above.>>>\n",
    "\n",
    "Step 1: Identify potential named entities\n",
    "- Amazon rainforest\n",
    "- jaguars\n",
    "- giant water lilies\n",
    "- Andes Mountains\n",
    "\n",
    "Step 2: Categorize each entity and asign a label\n",
    "- Amazon rainforest: biome (rainforest is a type of biome)\n",
    "- jaguars: fauna (common name of an animal)\n",
    "- giant water lilies: flora (common name of a plant)\n",
    "- Andes Mountains: nat_landform (mountain range not influenced by people)\n",
    "\n",
    "Step 3: Format the output\n",
    "\n",
    "   {{\"person\": [],\n",
    "    \"location\": [],\n",
    "    \"fauna\": [\"jaguars\"],\n",
    "    \"flora\": [\"giant water lilies\"],\n",
    "    \"weather\": [],\n",
    "    \"organisation\": [],\n",
    "    \"biome\": [\"Amazon rainforest\"],\n",
    "    \"myth\": [],\n",
    "    \"hum_landform\": [],\n",
    "    \"nat_landform\": [\"Andes Mountains\"],\n",
    "    \"nat_phenomenon\": [],\n",
    "    \"land_cover\": [],\n",
    "   }}\n",
    "\n",
    "Example 2:\n",
    "Sentence: <<<Hurricane Katrina devastated New Orleans in 2005, causing widespread flooding and destruction.>>>\n",
    "\n",
    "Step 1: Identify potential named entities\n",
    "- Hurricane Katrina\n",
    "- New Orleans\n",
    "\n",
    "Step 2: Categorize each entity and asign a label\n",
    "- Hurricane's Katrina: weather (description of a weather event)\n",
    "- New Orleans: location (proper name of a city)\n",
    "\n",
    "Step 3: Format the output\n",
    "\n",
    "  { {\"person\": [],\n",
    "    \"location\": [\"New Orleans\"],\n",
    "    \"fauna\": [],\n",
    "    \"flora\": [],\n",
    "    \"weather\": [\"Hurricane Katrina\"],\n",
    "    \"organisation\": [],\n",
    "    \"biome\": [],\n",
    "    \"myth\": [],\n",
    "    \"hum_landform\": [],\n",
    "    \"nat_landform\": [],\n",
    "    \"nat_phenomenon\": [],\n",
    "    \"land_cover\": [],\n",
    "  }}\n",
    "\n",
    "Example 3:\n",
    "Sentence: <<<The Great Wall of China stretches across the Gobi Desert, where Bactrian camels roam freely.>>>\n",
    "\n",
    "Step 1: Identify potential named entities\n",
    "- Great Wall of China\n",
    "- Gobi Desert\n",
    "- Bactrian camels\n",
    "\n",
    "Step 2: Categorize each entity and asign a label\n",
    "- Great Wall of China: hum_landform (man-made structure influencing the landscape)\n",
    "- Gobi Desert: biome (desert is a type of biome)\n",
    "- Bactrian camels: fauna (common name of an animal)\n",
    "\n",
    "Step 3: Format the output\n",
    "\n",
    "   {{\"person\": [],\n",
    "    \"location\": [],\n",
    "    \"fauna\": [\"Bactrian camels\"],\n",
    "    \"flora\": [],\n",
    "    \"weather\": [],\n",
    "    \"organisation\": [],\n",
    "    \"biome\": [\"Gobi Desert\"],\n",
    "    \"myth\": [],\n",
    "    \"hum_landform\": [\"Great Wall of China\"],\n",
    "    \"nat_landform\": [],\n",
    "    \"nat_phenomenon\": [],\n",
    "    \"land_cover\": [],\n",
    "  }}\n",
    "\n",
    "Now, DO NOT take into account the previous examples, use them JUST AS A REFERENCE. \n",
    "\n",
    "The sentence for analyzing is:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person\n",
      "['Weber, Max', 'pepe']\n",
      "<class 'list'>\n",
      "location\n",
      "['pep']\n",
      "<class 'list'>\n",
      "fauna\n",
      "[]\n",
      "<class 'list'>\n",
      "flora\n",
      "[]\n",
      "<class 'list'>\n",
      "weather\n",
      "[]\n",
      "<class 'list'>\n",
      "organisation\n",
      "[]\n",
      "<class 'list'>\n",
      "biome\n",
      "[]\n",
      "<class 'list'>\n",
      "myth\n",
      "[]\n",
      "<class 'list'>\n",
      "hum_landform\n",
      "[]\n",
      "<class 'list'>\n",
      "nat_landform\n",
      "[]\n",
      "<class 'list'>\n",
      "nat_phenomenon\n",
      "[]\n",
      "<class 'list'>\n",
      "land_cover\n",
      "[]\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "ent_dict={'person': ['Weber, Max','pepe'], 'location': ['pep'], 'fauna': [], 'flora': [], 'weather': [], 'organisation': [], 'biome': [], 'myth': [], 'hum_landform': [], 'nat_landform': [], 'nat_phenomenon': [], 'land_cover': []}\n",
    "\n",
    "def extract_entitites(ent_dict):\n",
    "    all_values = []\n",
    "    list_category = []\n",
    "    for category,value_list in ent_dict.items():\n",
    "            print(category)\n",
    "            print(value_list)\n",
    "            print(type(value_list))\n",
    "            if(value_list!=[]):\n",
    "                all_values.append(value_list)\n",
    "                list_category.append(category)\n",
    "\n",
    "    return all_values, list_category\n",
    "\n",
    "a,b=extract_entitites(ent_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Weber, Max', 'pepe'], ['pep']]\n"
     ]
    }
   ],
   "source": [
    "print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = [item for sublist in a for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Weber, Max', 'pepe', 'pep']\n"
     ]
    }
   ],
   "source": [
    "print(flat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Weber', ',', 'Max'], ['pepe']]\n"
     ]
    }
   ],
   "source": [
    "if a and isinstance(a[0], list):  # Verifica que a[0] sea una lista\n",
    "    ent_ok = [\n",
    "        re.findall(r\"\\w+|[^\\w\\s]\", elemento) for elemento in a[0] if isinstance(elemento, str)\n",
    "    ]\n",
    "    print(ent_ok)\n",
    "else:\n",
    "  print('n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m ent_ok \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfindall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mw+|[^\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43ms]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/neurips-llm/lib/python3.10/re.py:240\u001b[0m, in \u001b[0;36mfindall\u001b[0;34m(pattern, string, flags)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfindall\u001b[39m(pattern, string, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    233\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a list of all non-overlapping matches in the string.\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \n\u001b[1;32m    235\u001b[0m \u001b[38;5;124;03m    If one or more capturing groups are present in the pattern, return\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    238\u001b[0m \n\u001b[1;32m    239\u001b[0m \u001b[38;5;124;03m    Empty matches are included in the result.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfindall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "import re\n",
    "ent_ok = re.findall(r\"\\w+|[^\\w\\s]\", a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lieut', '-']\n"
     ]
    }
   ],
   "source": [
    "print(ent_ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "ent_ok_filtrada = [\n",
    "                elemento\n",
    "                for elemento in ent_ok\n",
    "                if elemento not in string.punctuation or elemento in [\".\", \";\", \",\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lieut']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ent_ok_filtrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre: one_shot, Contenido: Task: Extract relevant named entities from the given sentence based on the following labels:\n",
      "- person: proper names of people\n",
      "- location: proper names of locations\n",
      "- fauna: common and scientific names of animals and fauna\n",
      "- flora: common and scientific names of plants and flora\n",
      "- weather: descriptions of weather phenomena and climate\n",
      "- organisation: proper names of organizations\n",
      "- biome: names of biomes\n",
      "- myth: fantastical entities, deities, or God\n",
      "- hum_landform: landforms influenced by people\n",
      "- nat_landform: landforms not influenced by people\n",
      "- nat_phenomenon: dynamic and naturally occurring phenomena\n",
      "- land_cover: elements that cover the land\n",
      "\n",
      "Only respond in JSON format, Do not add any more comment.\n",
      "The structure of the JSON format is like this:\n",
      "{\n",
      "        \"person\": [\"entity_1\", \"entity_2\", \"entity_n\"],\n",
      "        \"location\": [\"entity_1\", \"entity_2\", \"entity_n\"],\n",
      "        \"fauna\": [\"entity_1\", \"entity_2\", \"entity_n\"],\n",
      "        \"flora\": [\"entity_1\", \"entity_2\", \"entity_n\"],\n",
      "        \"weather\": [\"entity_1\", \"entity_2\", \"entity_n\"],\n",
      "        \"organisation\": [\"entity_1\", \"entity_2\", \"entity_n\"],\n",
      "        \"biome\": [\"entity_1\", \"entity_2\", \"entity_n\"],\n",
      "        \"myth\": [\"entity_1\", \"entity_2\", \"entity_n\"],\n",
      "        \"hum_landform\": [\"entity_1\", \"entity_2\", \"entity_n\"],\n",
      "        \"nat_landform\": [\"entity_1\", \"entity_2\", \"entity_n\"],\n",
      "        \"nat_phenomenon\": [\"entity_1\", \"entity_2\", \"entity_n\"],\n",
      "        \"land_cover\": [\"entity_1\", \"entity_2\", \"entity_n\"],\n",
      "        }\n",
      "        \n",
      "\n",
      "Nombre: few_shot, Contenido: Task: Extract relevant named entities from the given sentence based on the following labels:\n",
      "- person: proper names of people\n",
      "- location: proper names of locations\n",
      "- fauna: common and scientific names of animals and fauna\n",
      "- flora: common and scientific names of plants and flora\n",
      "- weather: descriptions of weather phenomena and climate\n",
      "- organisation: proper names of organizations\n",
      "- biome: names of biomes\n",
      "- myth: fantastical entities, deities, or God\n",
      "- hum_landform: landforms influenced by people\n",
      "- nat_landform: landforms not influenced by people\n",
      "- nat_phenomenon: dynamic and naturally occurring phenomena\n",
      "- land_cover: elements that cover the land\n",
      "\n",
      "Only respond in JSON format, Do not add any more comment.\n",
      "The structure of the JSON format is like this:\n",
      "{\n",
      "        \"person\": [\"entity_1\", \"entity_2\", \"entity_n\"],\n",
      "        \"location\": [\"entity_1\", \"entity_2\", \"entity_n\"],\n",
      "        \"fauna\": [\"entity_1\", \"entity_2\", \"entity_n\"],\n",
      "        \"flora\": [\"entity_1\", \"entity_2\", \"entity_n\"],\n",
      "        \"weather\": [\"entity_1\", \"entity_2\", \"entity_n\"],\n",
      "        \"organisation\": [\"entity_1\", \"entity_2\", \"entity_n\"],\n",
      "        \"biome\": [\"entity_1\", \"entity_2\", \"entity_n\"],\n",
      "        \"myth\": [\"entity_1\", \"entity_2\", \"entity_n\"],\n",
      "        \"hum_landform\": [\"entity_1\", \"entity_2\", \"entity_n\"],\n",
      "        \"nat_landform\": [\"entity_1\", \"entity_2\", \"entity_n\"],\n",
      "        \"nat_phenomenon\": [\"entity_1\", \"entity_2\", \"entity_n\"],\n",
      "        \"land_cover\": [\"entity_1\", \"entity_2\", \"entity_n\"],\n",
      "        }\n",
      "        \n",
      " \n",
      "Here you have some examples:\n",
      "Example1:\n",
      "Sentence: <<<Hurricane Katrina devastated New Orleans in 2005, causing widespread flooding and destruction.>>>\n",
      "Output:\n",
      "   {\"person\": [],\n",
      "    \"location\": [\"New Orleans\"],\n",
      "    \"fauna\": [],\n",
      "    \"flora\": [],\n",
      "    \"weather\": [\"Hurricane\\'s' Katrina\"],\n",
      "    \"organisation\": [],\n",
      "    \"biome\": [],\n",
      "    \"myth\": [],\n",
      "    \"hum_landform\": [],\n",
      "    \"nat_landform\": [],\n",
      "    \"nat_phenomenon\": ['flooding'],\n",
      "    \"land_cover\": [],\n",
      "  }\n",
      "  \n",
      "Example 2:\n",
      "Sentence: <<<The Great Wall of China stretches across the Gobi Desert, where Bactrian camels roam freely.>>>\n",
      "Output:\n",
      "   {\"person\": [],\n",
      "    \"location\": [],\n",
      "    \"fauna\": [\"Bactrian camels\"],\n",
      "    \"flora\": [],\n",
      "    \"weather\": [],\n",
      "    \"organisation\": [],\n",
      "    \"biome\": [\"Gobi Desert\"],\n",
      "    \"myth\": [],\n",
      "    \"hum_landform\": [\"Great Wall of China\"],\n",
      "    \"nat_landform\": [],\n",
      "    \"nat_phenomenon\": [],\n",
      "    \"land_cover\": [],\n",
      "  } \n",
      "Example 3:\n",
      "Sentence: <<<The Amazon rainforest is home to jaguars and giant water lilies, while the Andes Mountains tower above.>>>\n",
      "Output:\n",
      "   {\"person\": [],\n",
      "    \"location\": [],\n",
      "    \"fauna\": [\"jaguars\"],\n",
      "    \"flora\": [\"giant water lilies\"],\n",
      "    \"weather\": [],\n",
      "    \"organisation\": [],\n",
      "    \"biome\": [\"Amazon rainforest\"],\n",
      "    \"myth\": [],\n",
      "    \"hum_landform\": [],\n",
      "    \"nat_landform\": [\"Andes Mountains\"],\n",
      "    \"nat_phenomenon\": [],\n",
      "    \"land_cover\": [],\n",
      "   }\n",
      "  \n",
      "Nombre: chain_of_thought, Contenido: Task: Extract relevant named entities from the given sentence based on the following labels:\n",
      "- person: proper names of people\n",
      "- location: proper names of locations\n",
      "- fauna: common and scientific names of animals and fauna\n",
      "- flora: common and scientific names of plants and flora\n",
      "- weather: descriptions of weather phenomena and climate\n",
      "- organisation: proper names of organizations\n",
      "- biome: names of biomes\n",
      "- myth: fantastical entities, deities, or God\n",
      "- hum_landform: landforms influenced by people\n",
      "- nat_landform: landforms not influenced by people\n",
      "- nat_phenomenon: dynamic and naturally occurring phenomena\n",
      "- land_cover: elements that cover the land\n",
      "\n",
      "Only respond in JSON format, Do not add any more comment.\n",
      "The structure of the JSON format is like this:\n",
      "{\n",
      "        \"person\": [\"entity_1\", \"entity_2\", \"entity_n\"],\n",
      "        \"location\": [\"entity_1\", \"entity_2\", \"entity_n\"],\n",
      "        \"fauna\": [\"entity_1\", \"entity_2\", \"entity_n\"],\n",
      "        \"flora\": [\"entity_1\", \"entity_2\", \"entity_n\"],\n",
      "        \"weather\": [\"entity_1\", \"entity_2\", \"entity_n\"],\n",
      "        \"organisation\": [\"entity_1\", \"entity_2\", \"entity_n\"],\n",
      "        \"biome\": [\"entity_1\", \"entity_2\", \"entity_n\"],\n",
      "        \"myth\": [\"entity_1\", \"entity_2\", \"entity_n\"],\n",
      "        \"hum_landform\": [\"entity_1\", \"entity_2\", \"entity_n\"],\n",
      "        \"nat_landform\": [\"entity_1\", \"entity_2\", \"entity_n\"],\n",
      "        \"nat_phenomenon\": [\"entity_1\", \"entity_2\", \"entity_n\"],\n",
      "        \"land_cover\": [\"entity_1\", \"entity_2\", \"entity_n\"],\n",
      "        }\n",
      "        \n",
      "\n",
      "Let's approach this step-by-step:\n",
      "Example 1:\n",
      "Sentence: <<<The Amazon rainforest is home to jaguars and giant water lilies, while the Andes Mountains tower above.>>>\n",
      "\n",
      "Step 1: Identify potential named entities\n",
      "- Amazon rainforest\n",
      "- jaguars\n",
      "- giant water lilies\n",
      "- Andes Mountains\n",
      "\n",
      "Step 2: Categorize each entity and asign a label\n",
      "- Amazon rainforest: biome (rainforest is a type of biome)\n",
      "- jaguars: fauna (common name of an animal)\n",
      "- giant water lilies: flora (common name of a plant)\n",
      "- Andes Mountains: nat_landform (mountain range not influenced by people)\n",
      "\n",
      "Step 3: Format the output\n",
      "\n",
      "   {\"person\": [],\n",
      "    \"location\": [],\n",
      "    \"fauna\": [\"jaguars\"],\n",
      "    \"flora\": [\"giant water lilies\"],\n",
      "    \"weather\": [],\n",
      "    \"organisation\": [],\n",
      "    \"biome\": [\"Amazon rainforest\"],\n",
      "    \"myth\": [],\n",
      "    \"hum_landform\": [],\n",
      "    \"nat_landform\": [\"Andes Mountains\"],\n",
      "    \"nat_phenomenon\": [],\n",
      "    \"land_cover\": [],\n",
      "   }\n",
      "\n",
      "Example 2:\n",
      "Sentence: <<<Hurricane Katrina devastated New Orleans in 2005, causing widespread flooding and destruction.>>>\n",
      "\n",
      "Step 1: Identify potential named entities\n",
      "- Hurricane Katrina\n",
      "- New Orleans\n",
      "\n",
      "Step 2: Categorize each entity and asign a label\n",
      "- Hurricane's Katrina: weather (description of a weather event)\n",
      "- New Orleans: location (proper name of a city)\n",
      "\n",
      "Step 3: Format the output\n",
      "\n",
      "  {'person': [], 'location': ['New Orleans'], 'fauna': [], 'flora': [], 'weather': ['Hurricane Katrina'], 'organisation': [], 'biome': [], 'myth': [], 'hum_landform': [], 'nat_landform': [], 'nat_phenomenon': [], 'land_cover': []}\n",
      "\n",
      "Example 3:\n",
      "Sentence: <<<The Great Wall of China stretches across the Gobi Desert, where Bactrian camels roam freely.>>>\n",
      "\n",
      "Step 1: Identify potential named entities\n",
      "- Great Wall of China\n",
      "- Gobi Desert\n",
      "- Bactrian camels\n",
      "\n",
      "Step 2: Categorize each entity and asign a label\n",
      "- Great Wall of China: hum_landform (man-made structure influencing the landscape)\n",
      "- Gobi Desert: biome (desert is a type of biome)\n",
      "- Bactrian camels: fauna (common name of an animal)\n",
      "\n",
      "Step 3: Format the output\n",
      "\n",
      "   {\"person\": [],\n",
      "    \"location\": [],\n",
      "    \"fauna\": [\"Bactrian camels\"],\n",
      "    \"flora\": [],\n",
      "    \"weather\": [],\n",
      "    \"organisation\": [],\n",
      "    \"biome\": [\"Gobi Desert\"],\n",
      "    \"myth\": [],\n",
      "    \"hum_landform\": [\"Great Wall of China\"],\n",
      "    \"nat_landform\": [],\n",
      "    \"nat_phenomenon\": [],\n",
      "    \"land_cover\": [],\n",
      "  }\n",
      "\n",
      "Now, DO NOT take into account the previous examples, use them JUST AS A REFERENCE. \n",
      "\n",
      "The sentence for analyzing is:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompts=[one_shot,few_shot,chain_of_thought]\n",
    "prompts_name=['one_shot','few_shot','chain_of_thought']\n",
    "for i,value in enumerate(prompts):\n",
    "    contenido_prompt_activo = value\n",
    "    nombre_prompt = prompts_name[i]\n",
    "    print(f\"Nombre: {nombre_prompt}, Contenido: {contenido_prompt_activo}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_dict = {\n",
    "    \"person\": [\"Weber, Max\", \"Durkheim, Émile\"],\n",
    "    \"place\": [\"France\"],\n",
    "    \"organization\": [\"UNESCO\", \"World Bank\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(ent_dict):\n",
    "    all_values = []\n",
    "    list_category = []\n",
    "    \n",
    "    for category, value_list in ent_dict.items():\n",
    "        # Convertir valores únicos a listas\n",
    "        if isinstance(value_list, str):\n",
    "            value_list = [value_list]\n",
    "        \n",
    "        if value_list:  # Si no está vacío\n",
    "            for value in value_list:\n",
    "                all_values.append(str(value))  # Agregar el valor a all_values como cadena\n",
    "                list_category.append(category)  # Agregar la categoría correspondiente\n",
    "    \n",
    "    return all_values, list_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b=extract_entities(ent_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Weber, Max', 'Durkheim, Émile', 'France', 'UNESCO', 'World Bank']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_1 = [\"plankton\"]\n",
    "sentence_1 = [\"this\", \"plankton\", \"last\", \"plankton\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sub_list(ent_1,sentence_1):\n",
    "    results=[]\n",
    "    sll=len(ent_1)\n",
    "    for ind in (i for i,e in enumerate(sentence_1) if e.upper()==ent_1[0].upper()):\n",
    "        if ind+sll<=len(sentence_1) and [elem.upper() for elem in sentence_1[ind:ind+sll]] == [elem.upper() for elem in ent_1]:\n",
    "         results.append((ind,ind+sll-1))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=find_sub_list(ent_1,sentence_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1), (3, 3)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_O = [[\"O\"] * len(sentence_1)][0]\n",
    "\n",
    "sentence_2 = [[\"O\"] * len(sentence_1)][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'O', 'O', 'O']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1)\n",
      "oo [1]\n",
      "(3, 3)\n",
      "oo [3]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "for result in a:\n",
    "    print(result)\n",
    "    all_indices = list(range(result[0], result[-1] + 1))\n",
    "    print('oo',all_indices)\n",
    "    # index of start aspect = B-aspect\n",
    "    start_index = all_indices[0]\n",
    "    sentence_O[start_index] = \"B-ASPECT\"\n",
    "    sentence_2[start_index] = f\"B-PEPE\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neurips-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
